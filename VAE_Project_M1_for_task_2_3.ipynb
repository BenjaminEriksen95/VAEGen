{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "name": "VAE_Project_M1_for_task_2-3.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "otvvAip7Kko3"
      },
      "source": [
        "# Semi-Supervised VAE Project\n",
        "## By VAEGen"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s_6l2tM9Kko4"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LE0_IEDSKko4"
      },
      "source": [
        "### Import libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cReuSxE4Kko4"
      },
      "source": [
        "import os\n",
        "import time\n",
        "import torch\n",
        "import argparse\n",
        "from functools import reduce\n",
        "# \n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import defaultdict\n",
        "\n",
        "# Torch\n",
        "import torch\n",
        "from torch.distributions import Bernoulli\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data.sampler import SubsetRandomSampler\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Torchvision\n",
        "import torchvision\n",
        "from torchvision import transforms\n",
        "from torchvision.datasets import MNIST\n",
        "from torchvision.datasets import FashionMNIST\n",
        "from torchvision.transforms import ToTensor"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VHgNhco8Kko5"
      },
      "source": [
        "### Meta parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d11hjw8zKko5"
      },
      "source": [
        "batch_size=32*8*2\n",
        "epochs=5\n",
        "learning_rate=1e-3\n",
        "latent_dim = 8\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "device='cpu'\n",
        "#print layers, used for debugging\n",
        "NNprint_ = False\n",
        "classes=[0,1,2,3,4]\n",
        "num_classes=len(classes)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eauQ5ER9Kko5"
      },
      "source": [
        "### Import MNIST Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ONqyOK32Kko5"
      },
      "source": [
        "# transforms arrays into tensors, can be extended\n",
        "transformations = transforms.Compose([\n",
        "                          transforms.ToTensor(),\n",
        "                          transforms.Lambda(lambda p: Bernoulli(probs=p).sample())])\n",
        "\n",
        "# define the train and test sets\n",
        "dset_train = MNIST(root='data', train=True,  transform=transformations, download=True)\n",
        "dset_test  = MNIST(root='data', train=False, transform=transformations)\n",
        "\n",
        "def stratified_sampler(labels,classes):\n",
        "    \"\"\"Sampler that only picks datapoints corresponding to the specified classes\"\"\"\n",
        "    (indices,) = np.where(reduce(lambda x, y: x | y, [labels.numpy() == i for i in classes]))\n",
        "    indices = torch.from_numpy(indices)\n",
        "    return SubsetRandomSampler(indices)\n",
        "\n",
        "# The loaders perform the actual work\n",
        "train_loader = DataLoader(dset_train, batch_size=batch_size,\n",
        "                          sampler=stratified_sampler(dset_train.targets,classes))\n",
        "test_loader  = DataLoader(dset_test, batch_size=batch_size, \n",
        "                          sampler=stratified_sampler(dset_test.targets,classes))"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p-6CSXtQKko5"
      },
      "source": [
        "### Import FashionMNIST Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "FEOJn9eNKko5"
      },
      "source": [
        "train = FashionMNIST(\n",
        "    root='data', train=True, transform=transforms.ToTensor(),\n",
        "    download=True)\n",
        "test = FashionMNIST(\n",
        "    root='data', train=False, transform=transforms.ToTensor(),\n",
        "    download=False)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iBq7eFyCKko5"
      },
      "source": [
        "### Configure Loaders"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "scJb6mLOKko5"
      },
      "source": [
        "train_loader = torch.utils.data.DataLoader(\n",
        "  train,\n",
        "  batch_size=batch_size,sampler=stratified_sampler(train.targets,classes))\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "  test,\n",
        "  batch_size=batch_size,sampler=stratified_sampler(test.targets,classes))"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oI7Erbs8Kko5"
      },
      "source": [
        "### Plot examples"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fYOI8c1KKko5"
      },
      "source": [
        "\n",
        "examples = enumerate(test_loader)\n",
        "batch_idx, (example_data, example_targets) = next(examples)\n",
        "def plot_examples(model=None,sample=False,data=None):\n",
        "    if data is None:\n",
        "        if (model==None):\n",
        "            batch_idx, (data, example_targets) = next(examples)\n",
        "        else:\n",
        "            batch_idx, (data, example_targets) = next(examples)\n",
        "            if sample:\n",
        "                zeros=torch.zeros(num_classes,1024)\n",
        "                samples= torch.cat((torch.rand(latent_dim,1024),zeros)).T\n",
        "                data = model.sample(samples.to(device))\n",
        "            else:\n",
        "                data = model(data.to(device),example_targets)[0]\n",
        "        \n",
        "    fig = plt.figure()\n",
        "    fig.set_figheight(15)\n",
        "    fig.set_figwidth(15)\n",
        "    for i in range(64):\n",
        "        plt.subplot(8,8,i+1)\n",
        "        plt.tight_layout()\n",
        "        if model:\n",
        "            plt.imshow(data[i][0].cpu().data, cmap='gray', interpolation='none')\n",
        "        else:\n",
        "            plt.imshow(data[i][0], cmap='gray', interpolation='none')\n",
        "    #     plt.title(\"Ground Truth: {}\".format(example_targets[i]))\n",
        "        plt.xticks([])\n",
        "        plt.yticks([])"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "wvcJsHB3Kko5",
        "outputId": "5d142d88-219c-439b-d184-388ec3772ef5"
      },
      "source": [
        "plot_examples()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAABCgAAAQNCAYAAACfCXt5AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nOzdTXbcOrIuUOVbnsJp1/yH5b7nwNso+1U6j8hMkAA//OzdqlqSUzxCEKRiRQQe27Z9AQAAACT9v/QFAAAAAEhQAAAAAHESFAAAAECcBAUAAAAQJ0EBAAAAxElQAAAAAHE/Sr758Xg4k7RD27Y90tcgNvokNtgjNtgjNtgjNjjwa9u2f5IXIDa6JTbY821sqKAAAACu+Jm+ALolNtjzbWxIUAAAAABxEhQAAABAnAQFAAAAECdBAQAAAMRJUAAAAABxEhQAAABAnAQFAAAAECdBAQAAAMT9SF8AAACwb9u23a89Ho8brwSgLRUUAAAAQJwEBQAAABAnQQEAAADEmUHxgaO+v1f6ANemRxQAuKrk3dO7x1pKYuOVeBjDlTWuIR0nKigAAACAOAkKAAAAIE6LBxRIl1wB8LnXPfu5bPXdfn5U4lryua+f8/z1dBktfWnxjvEuHoF71Lq/j54ptaT3DRUUAAAAQJwEBQAAABAnQQEAAADEmUHxDceK8uyoX9hMij5dOXJNf/i6Wu396V5O/qfW0Y0l33v2a++Io/HU3AtK5qkwvlpr7B3nPnfdlyXzkkahggIAAACIk6AAAAAA4rR4/DZqCQz3ulICrqyuDzVLvK3j+M7u/VeeGdoIqcEzZXy19gLtp/O5Yw21H7ZV676s1Qr2qud9QgUFAAAAECdBAQAAAMRJUAAAAABxZlCwvFY9WHr7cq4cAcn4ZlpTcwauadWbr+efM97Fzdl7XPyN6ez+7h1nTJ7hn1NBAQAAAMRJUAAAAABxEhQAAABAnBkUH9AzNJ+jvr87ZlKIqX5c6eW0puN7Xrd3/eDpXt53P18Mvley3kfSsfD1Zb1nYA3XcrRv9LCnML9R4kwFBQAAABAnQQEAAADEafH4gOMi53al3Km3EnDg/B797t+12vvtGxnv1jOxLkfXJE549mk8eGe9V6v7dLQWNDJmWXsVFAAAAECcBAUAAAAQJ0EBAAAAxC07g6KkR0f/3hiO1vTKGrboCTbXBNblKGP2HD0brCn07+go47Of884scwc4p8X6p583KigAAACAOAkKAAAAIE6CAgAAAIhbZgaF/qz5HfX93XUutTgbX601bDUThXbezYYpmfFwxzwIcdRWrV7yK+wj/OH9ok+11qXmrDSxMpe7/obpiQoKAAAAIE6CAgAAAIhbpsUDnh2V7t5R8tRzWRWf66EEnHaO1rRkvd9976dHGds3eKb9gz3Wv093rYt3E0anggIAAACIk6AAAAAA4iQoAAAAgDgzKJjG2Z5wvZrsKYmNkmMnE3NP6EfNuOIerY6VbdUfbo8Zk3kB/StZI/cdZ53dC2aJORUUAAAAQJwEBQAAABAnQQEAAADETT2DYvX+ndld6dVMrLG4GoN1WlfJnIGan8v4zq7xldgwd+k+vc+GsKYwltH+hrmbCgoAAAAgToICAAAAiJu6xQP+aFUO1XvZJ1DP0T7iWEda87ypa7Tfpz0FxjLaHtMTFRQAAABAnAQFAAAAECdBAQAAAMRNNYNCr8/c3q3vHf2ZJTGmX3RtR7EiNubnWEdqe40j7zz3KbmHrct8Rnv3+/R6e7jW0bW631dfGxUUAAAAQJwEBQAAABAnQQEAAADETTWD4orVe316VdLH36rnWz8pZxzF52tM2X/mZr2p4cqzSMz9V4ve/LveEewj95l15sTXVx/XO5o77vHEuvS8p6igAAAAAOIkKAAAAIC4oVs8lDuu7UppUq1yLXHEGeJmfFq/aMGzqU93HSVoXxlPz2Xy9OuuOBl1T1FBAQAAAMRJUAAAAABxEhQAAABA3NAzKK7QMza3VusrTviUPWZ85hytreSY6xY/g5x36/K8/jX3+qPPfdXqaHXMBqG9K3tMKz3tKSooAAAAgDgJCgAAACBuuBaPs2Uu6VIVPnOljOno3yrPozUxxrOeSiX5Xs17NnH/i6tyR7+zsy09PazDHe1IK2n1LppoHbH+16XbLe7SU6yooAAAAADiJCgAAACAOAkKAAAAIG64GRSJPiDGd9T311PPFfe7ckScPWh8rdbQvtKnWuu9en/wjHr7/V6ZV9Dbf8toWs2K8LwZn3fEe6igAAAAAOIkKAAAAIA4CQoAAAAgbrgZFHrw5nb2jPKaPwf2OGu+TyVzRPQA0zNxxB6xkdPD/DvrP57Eu0jJNfRMBQUAAAAQJ0EBAAAAxA3X4jFqqQrXWXvu5oiofn26NlfW0J4znztKtd8dUSiuYFzuX2oQR8dUUAAAAABxEhQAAABAnAQFAAAAEDfcDAqAmt71ix99Lzm1ZglY03XdtfZiDAA+p4ICAAAAiJOgAAAAAOIkKAAAAIA4MygAnugXH8/RmllPAIBxqKAAAAAA4iQoAAAAgDgJCgAAACBOggIAAACIk6AAAAAA4iQoAAAAgLjSY0Z/fX19/WxxIZz2n/QF/CY2+iM22CM22CM22CM2ONJDfIiNPokN9nwbG49t2+6+EAAAAIC/aPEAAAAA4iQoAAAAgDgJCgAAACBOggIAAACIk6AAAAAA4iQoAAAAgDgJCgAAACBOggIAAACIk6AAAAAA4iQoAAAAgDgJCgAAACBOggIAAACIk6AAAAAA4iQoAAAAgDgJCgAAACBOggIAAACIk6AAAAAA4iQoAAAAgDgJCgAAACBOggIAAACIk6AAAAAA4iQoAAAAgDgJCgAAACBOggIAAACIk6AAAAAA4iQoAAAAgDgJCgAAACBOggIAAACI+1HyzY/HY2t1IZy3bdsjfQ1io09igz1igz1igz1igwO/tm37J3kBYqNbYoM938aGCgoAAOCKn+kLoFtigz3fxoYEBQAAABAnQQEAAADESVAAAAAAcRIUAAAAQJwEBQAAABAnQQEAAADESVAAAAAAcRIUAAAAQNyP9AUAAHBs27bdrz0ejxuvBADaUUEBAAAAxElQAAAAAHESFAAAAEDcMjMojno339Hbyac+jTMxBXN73gte7/eS55G9gj+O4ug1psQNZJydFfPuufD8b0u+F0akggIAAACIk6AAAAAA4qZu8bjS1nH0OUqnANZW8nyp9SxibeJoDK3WybvnGI5aMa7ERq1njjhiBCooAAAAgDgJCgAAACBOggIAAACIm2oGxV39mUfHx7EWfX7jGX2mjP0HeGf0fY5/876xlpI1PTufQtysZ5T1V0EBAAAAxElQAAAAAHFDt3j0cOSWkjvoXw97xVlH166Mu627jgscOT6p6+w7hb0gp4f71/r3o+QerlVub2/gjJ7/hlVBAQAAAMRJUAAAAABxEhQAAABAXPczKEp6+/T10oI44k7ibX6OhOOPd7Fwds31nbfV+z5t3xhDYm3ERh+u/H3b48+pTQUFAAAAECdBAQAAAMRJUAAAAABx3c+gKNFbT2Cr3lL6ZU370Nte8E7PZ1Gv7MqZ9bVi0PrPp8X+JE7aGu2Z8sy7aFu9P7+PrsGsmn7Vmj80KhUUAAAAQJwEBQAAABDXXYvHXaUpdx3Zwnis73ye7/ce1vfsMZOvlGPeq6RUtsXPYAytjnUb9bi4EdV8TpzdN+56T3XsZJl3bRHPX9dCQc96jkcVFAAAAECcBAUAAAAQJ0EBAAAAxHU3g2IEtfrZ9f2Nz7r14d192MPcibPEGPQvPR/CPnHdTHNkzr6nmplQzpwoPtXqXTT9/GlBBQUAAAAQJ0EBAAAAxElQAAAAAHFTzaAYpa/mj9GuF2vWk9F67j693h6ulXKv63a211QPeJ9K1qXVmh3FmLgpN/JsolbEzXW9/w6P4t4+knPX7/rT+YfpWFBBAQAAAMRJUAAAAABxU7V4JFwp63XMaB8cwUULSvzXVus4anJ6K4V2nGGfrvzurdt8en+3L3k29f7f0rse/r4oaePoiQoKAAAAIE6CAgAAAIiToAAAAADiuphB0XMPTCl9x2M4uzZ68NZ2xz0txnimB3ht5k5A3kz34WhHtK+q1vvmqH/vqKAAAAAA4iQoAAAAgDgJCgAAACAuMoOiVl9Nuj/mO+ZOzKfHOKMdcyao7XW99QCPr9W6zNTrPrNZ1+I1/mb97yx1NF/O74xZ9BS7KigAAACAOAkKAAAAIC7S4nFUQjJai8SV6+2plGZ2SqrXNtq+Qj2j3/uOGe3DXb/7K+1A8B0x1dZo+/JRu8orz59rSn5nJa1Co7/XfEIFBQAAABAnQQEAAADESVAAAAAAcUMfM5o62qfF9Y/aI9SrFfqzYGX6qPnElWdB4ihR7jPzbAbvl/cZ7ZhR78d9mmk+Yw0qKAAAAIA4CQoAAAAgToICAAAAiIvMoOi97++u69HbBeeVnOXdA/d7XSOt/5XrEzfj0/NNa0cx1vv+SFv2n7XMsoYqKAAAAIA4CQoAAAAgLtLi8SpRqnt0LFDvLSh8Txnbumres7WOehJjbdmX+UStveHdvyt5j7E3UEOLPVBsvvduT+nhiFfPR0anggIAAACIk6AAAAAA4iQoAAAAgLguZlDU6pW68jl39Gvp7YN71LrX9JL3q8XsoqPZRFc/61Niqq6Sdej9HYK2SvaUo69fmXtSawYS90nMrav1rPK8oVcqKAAAAIA4CQoAAAAgrosWj8Qxo60ol7qPIx9pzZHDY2i1Tq3KuN99FuOxhvzRe6uQWL1PSSvGu+89ig3vJsxGBQUAAAAQJ0EBAAAAxElQAAAAAHFdzKB4NlrPt14+GFutfmF7QT+O5hrVesY4gnYt1nMto72LvhKvOUfPn7uOPS55BtKn1efsqaAAAAAA4iQoAAAAgDgJCgAAACCuuxkUr3rsA5yx1wd4z70/nlbPELEwButEDUdxlHovFdv9K1mjVjMHxAkjUkEBAAAAxElQAAAAAHHdt3i8UqrEH2IBKGXfAGp6t6ccHUd9VNZvr1qL9eZTK8SKCgoAAAAgToICAAAAiJOgAAAAAOKGm0EBcJcV+vwAaOfoOeIZA3zneW9IHWWcpIICAAAAiJOgAAAAAOIkKAAAAIA4MyiApekBBgCgRyu+p6qgAAAAAOIkKAAAAIA4CQoAAAAgToICAAAAiJOgAAAAAOIkKAAAAIC40mNGf319ff1scSGc9p/0BfwmNvojNtgjNtgjNtgjNjjSQ3yIjT6JDfZ8GxuPbdvuvhAAAACAv2jxAAAAAOIkKAAAAIA4CQoAAAAgToICAAAAiJOgAAAAAOIkKAAAAIA4CQoAAAAgToICAAAAiJOgAAAAAOIkKAAAAIA4CQoAAAAgToICAAAAiJOgAAAAAOIkKAAAAIA4CQoAAAAgToICAAAAiJOgAAAAAOIkKAAAAIA4CQoAAAAgToICAAAAiJOgAAAAAOIkKAAAAIA4CQoAAAAgToICAAAAiJOgAAAAAOIkKAAAAIA4CQoAAAAgToICAAAAiPtR8s2Px2NrdSGct23bI30NYqNPYoM9YoM9YoM9YoMDv7Zt+yd5AWKjW2KDPd/GhgoKAADgip/pC6BbYoM938aGBAUAAAAQJ0EBAAAAxElQAAAAAHESFAAAAECcBAUAAAAQJ0EBAAAAxElQAAAAAHESFAAAAEDcj/QFAEAPtm1r8rmPx6PJ5zKGK3EldmAeJXuBe5+VqaAAAAAA4iQoAAAAgDgJCgAAACDODAoAKPSuP/i51/i171hv8dxqzjJ5/ixxMx8zCdbyuobub85YYa6RCgoAAAAgToICAAAAiNPiccLZ0ppRymo47yg2rP/crpTxH5V5ag+4T83f7fNntTq+lH7cscb2gjHVig3rP553a3b0nLC+a7nrPWGUtiIVFAAAAECcBAUAAAAQJ0EBAAAAxJlB8Y1WfUD6y+Z31E84St8Xn6u1V5TMKxBHYzB3gj3mk8zlrqNCxUqfzB6D+lRQAAAAAHESFAAAAEDcMi0erUrjjsq3lOOtx5rPpYdjn8TUGKzT2s6Wcrvfx+RoaO52dm8Qf+zpea9SQQEAAADESVAAAAAAcRIUAAAAQNwyMyhq6qlHh3td6dcSN3O7sr5iYzxmBaylx15dRw7nHP2+rcXc7pppV+vn9Lh34R3iHRUUAAAAQJwEBQAAABAnQQEAAADETTWDolY/z5X+LD1Fa7He8ylZU72ccxML/GF9uZv3izHV2iuOZlK8+xlip38lM0darXfPzzUVFAAAAECcBAUAAAAQN1WLxxV3lLn0XEpDHda4T3eUO9Y6yqvkWh0f1larY9/gDzEFY7vruVvr5zieuE/W4m8qKAAAAIA4CQoAAAAgToICAAAAiJt6BsVzP89dfZ5HP1O/OMxFLycAzO3ob4ge3+0Tf/9wnytr2kN8fkIFBQAAABAnQQEAAADESVAAAAAAccPNoCjpuzn63lY9OImfSV369dZ2dJ/20GtqH8nR10tP7AXz8Q45npHXpYd3Gt5b8X1DBQUAAAAQJ0EBAAAAxHXf4rFiWQv3WuG4HupItIYxhpI1tG+M70opvvudT9gngKtG3UdUUAAAAABxEhQAAABAnAQFAAAAENf9DIpa9I5Tw6i9XKsxD4DWrsSNo93GcPb53uq9QJyM711sWGNqe42poxj0bOrH6n9fqqAAAAAA4iQoAAAAgDgJCgAAACBuqhkUd/VKfdoXpHerX2dnFOjP69NovXpmZPDsOR6sd1uj7RXPjp4/nk1jKJkHQJ9Gu7e8b6xlljVUQQEAAADESVAAAAAAcd21ePRYitTjNVGmVhml9R1fag2V8kLeUVvEaEa//hVZM+6mrWgM1uVvKigAAACAOAkKAAAAIE6CAgAAAIjrbgZFD8ycWJs1HUNv9+mV/kExN7eS2HBcZF133Zfp/mFx0q+j2LBu45l5j575v20V755FoxxPrYICAAAAiJOgAAAAAOIkKAAAAIA4Myh+662fHfhbD/dorX49e8h8as0gEBs5o/3uR7veVZg5QU96eHfi31Jzi9Lzkj6lggIAAACIk6AAAAAA4oZr8ahVfjRKiQvnKGlbS801vHIk5BFxNj5tHON5/V0freFRC9dd7wxiYzytnhmvxEbO0V7Q81GNZI32t+bz9abjWAUFAAAAECdBAQAAAMRJUAAAAABxXcyguKNHR684z6zxXHro8xNTYyjpFzZzYnxX1vCOfUVsrKVkJsorsw768G4NE338Z/cqMdSvVjOQErOVzlBBAQAAAMRJUAAAAABxkRaPHksulTnNzfqO745S/JKfSb/uOvbvmVhhj9iYz9G+UbLeJd/bQysBmefLFWJjTK1ip9be1ZoKCgAAACBOggIAAACIk6AAAAAA4iIzKK4cs3T25zieaX499PqR8e5+1qu7lpK9/yg2RunVZF+t9S79OazjrrU/ilfvuPep+b5hBtLc7vp7t+QaRqGCAgAAAIiToAAAAADiJCgAAACAuMgMCj04nGHGBGe5/9f1bu2Pvi5u5mO9OaPHGQ89XAP/VrIu1pAaZowjFRQAAABAnAQFAAAAEBdp8TgyY5kKdTgCEAC4m3cK4Cr7yOdUUAAAAABxEhQAAABAnAQFAAAAEBeZQaEHhxrEEQAAwDxUUAAAAABxEhQAAABAnAQFAAAAECdBAQAAAMRJUAAAAABxEhQAAABAnAQFAAAAECdBAQAAAMRJUAAAAABxEhQAAABA3I/C7//19fX1s8WFcNp/0hfwm9joj9hgj9hgj9hgj9jgSA/xITb6JDbY821sPLZtu/tCAAAAAP6ixQMAAACIk6AAAAAA4iQoAAAAgDgJCgAAACBOggIAAACIk6AAAAAA4iQoAAAAgDgJCgAAACBOggIAAACIk6AAAAAA4iQoAAAAgDgJCgAAACBOggIAAACIk6AAAAAA4iQoAAAAgDgJCgAAACBOggIAAACIk6AAAAAA4iQoAAAAgDgJCgAAACBOggIAAACIk6AAAAAA4iQoAAAAgDgJCgAAACBOggIAAACIk6AAAAAA4iQoAAAAgDgJCgAAACDuR8k3Px6PrdWFcN62bY/0NYiNPokN9ogN9ogN9ogNDvzatu2f5AWIjW6JDfZ8GxsqKAAAgCt+pi+AbokN9nwbGxIUAAAAQJwEBQAAABAnQQEAAADESVAAAAAAcRIUAAAAQJwEBQAAABAnQQEAAADESVAAAAAAcRIUAAAAQJwEBQAAABAnQQEAAADESVAAAAAAcRIUAAAAQNyP9AUAjGLbtt2vPR6PG68EGJV9BAD2qaAAAAAA4iQoAAAAgDgtHh84KscsoXRzbc9xJBb6dfZ+f/fvrDln2Dfm87yOr/vG6/+35sAr+wSfOvtOm44pFRQAAABAnAQFAAAAECdBAQAAAMQtM4Oi1hwJ5tOql+8o5vQPrscsAfY4dpI99g3+GLWXnDq8U66rZMbZLH/vqqAAAAAA4iQoAAAAgLhlWjxgz2spnHLrudUsf5uxrG41vd/vSvznY69Yy9E9fEcslPwMewxkXNkLzt7jPbcNqaAAAAAA4iQoAAAAgDgJCgAAACBu2RkUR32AV2YS6C0dQ6t16r2ffVWOZ+MTPa53j9fEfaz/eEr6unsgxsZwNDvAGq6l1nr3/DesCgoAAAAgToICAAAAiJOgAAAAAOKWmUFxpV/nqEenp34d9vVwDvjRnBPgHj2f+/0d+wb05453v577w7nfaM8ujqX+Ljm7j9wdYyooAAAAgDgJCgAAACBumRaPd1qUqiixml8PrSP8213rYv3nYo1oQWk+nzrag87uT+Jvfp5d4+ttDdPXo4ICAAAAiJOgAAAAAOIkKAAAAIA4Myh+q9Wjl+7Z4b966LkUCzklx7OVHJ3UQ1yxFvsI3G/0vb7k+h1l3KfRY5Bjre61K3HT0/2vggIAAACIk6AAAAAA4iQoAAAAgLjhZlCc7a0p6Um/8rncJ7GGegLHVDJXwt5AbSUxJW4gI/F8d7+zp9XfLTACFRQAAABAnAQFAAAAEDdci8dZNUujlOSNz1GS/FErFuwLfTp73N53ntf43fG09hHOsI+MzxpSw9EzRIytbZajRI+ooAAAAADiJCgAAACAOAkKAAAAIG64GRRHPcAtfgb9utLzLXbYY3bAXGrOhjj6tyv0hPIZe0i/7jhG+t18mhbE3Pis4fye17jV3KpZ3idUUAAAAABxEhQAAABAnAQFAAAAEDfcDIpnV/ps9HqNzxpSgzhaS6u+Tzhjln5h/ueuNbV3zcWzaS0l67tibKigAAAAAOIkKAAAAIC4oVs8WkkcEUU5LT6c0Wrt332ufaRPrdbFHrMW6z2X1H6diCPPJrjH873mb81jKigAAACAOAkKAAAAIE6CAgAAAIgzg+Ib+oDmd9QHdvZz4Eit3mIxN4ZaewzzcQ/3r2Sm0Gj3t/iDvCv34Qr3sAoKAAAAIE6CAgAAAIiToAAAAADilplBUdIj6Gza+Z3tGRULY5j5Hi6J3Zn+u0dztE7WZXxX5g48/1uxcK+RZ0eUEFfzsabsmfF9QwUFAAAAECdBAQAAAMTd1uIxUklj79cHlOmhpeeOcmJ7F/TPfTq+mdtDyBBT8D8qKAAAAIA4CQoAAAAgToICAAAAiGs2g+Kol6rWMXk1+7X0hM7tSqyIjf6NsL7iCObxej/rHx/PXe+XrXimjG/G4yFnNNIcxVmooAAAAADiJCgAAACAuGotHq3K4Vp9rhId9oiNMYxQggvMyf4zt5L3gNdYKGn/Ofpe7yJrexdX3Of5d19rTEGJFZ83KigAAACAOAkKAAAAIE6CAgAAAIirNoPiSr9eyefoz6M2cTQ/awyk2H/m9m59S9ZfrPCHWOhTyYyZu+Yozvi3sQoKAAAAIE6CAgAAAIiToAAAAADiqs2gKHGlP2aW3hrgmqNzqe0T3O3KbCXGU9KHDMCczs5grPkMmfEdQwUFAAAAECdBAQAAAMRFWjwgTXvAXKwZkGQPAkodvYsyvit/X6z+TFFBAQAAAMRJUAAAAABxEhQAAABAnBkULOGol2v1Pi/gOvsIAGd5hszHzInzVFAAAAAAcRIUAAAAQJwEBQAAABAnQQEAAADESVAAAAAAcRIUAAAAQJwEBQAAABAnQQEAAADESVAAAAAAcRIUAAAAQNyPwu//9fX19bPFhXDaf9IX8JvY6I/YYI/YYI/YYI/Y4EgP8SE2+iQ22PNtbDy2bbv7QgAAAAD+osUDAAAAiJOgAAAAAOIkKAAAAIA4CQoAAAAgToICAAAAiJOgAAAAAOIkKAAAAIA4CQoAAAAgToICAAAAiJOgAAAAAOIkKAAAAIA4CQoAAAAgToICAAAAiJOgAAAAAOIkKAAAAIA4CQoAAAAgToICAAAAiJOgAAAAAOIkKAAAAIA4CQoAAAAgToICAAAAiJOgAAAAAOIkKAAAAIA4CQoAAAAgToICAAAAiJOgAAAAAOIkKAAAAIC4HyXf/Hg8tlYXwnnbtj3S1yA2+iQ22CM22CM22CM2OPBr27Z/khcgNrolNtjzbWyooAAAAK74mb4AuiU22PNtbEhQAAAAAHESFAAAAECcBAUAAAAQJ0EBAAAAxElQAAAAAHESFAAAAECcBAUAAAAQJ0EBAAAAxElQAAAAAHE/0hcAAABAuW3b/vr/j8cjdCVQhwoKAAAAIE6CAgAAAIiToAAAAADizKAAAIAPvfb8HzEPgDNKYuzo34o/RqSCAgAAAIiToAAAAADipmrxUHJHz5TcAe8cPcfsG/Oz/kCp173hSnsI9EAFBQAAABAnQQEAAADESVAAAAAAcVPNoChhXgVJr/EnxtZSsv5iBaBf7/r/zZ/ij1azIY4+11wbRqSCAgAAAIiToAAAAADilm3xKKE8anyJNRQ3PDtbgnn0veJoDvaKdb27963/eI5aPrTsUUNJ3Ii5tdVqK7o7blRQAAAAAHESFAAAAECcBAUAAAAQN9UMilr9Ma2OAeI+1pCzWvXrnd2fxHKO3l3uJsbGYJ2425W5E4yntzW8+31IBQUAAAAQJ0EBAAAAxElQAAAAAHFTzaC4ordeH+6jz3wtre7112coyGgAABsISURBVLipFVdHnyt27/X8+271u7aGa3NPwzoS9/cdzzHeS72LjkIFBQAAABAnQQEAAADELdPiMWqJC587u8Y1S9yOrkEpXU6ro0NrfS9r8Txam+cErMn9zaeeY+VK61+tlo+7Y1cFBQAAABAnQQEAAADESVAAAAAAcVPPoEj0nTMGazq+Vvf3SEdw9X59ozEbArjTUW+5I2fZc+VZJY76cNf8u5Jr6On9VwUFAAAAECdBAQAAAMRJUAAAAABxU82g0JO1lpL1LllffejzuXJmdNpRPOpRvq7VPlLCuq3L2s+v1vuH/Z4zxMmYSvaCEkfx0FOsqKAAAAAA4iQoAAAAgLipWjyY25WSph7aNo6uoaeyqhnM9Pvs+Rio2dX6/faw/5Bj/fnU0Z4jjtZm/Tlj1PdEFRQAAABAnAQFAAAAECdBAQAAAMSZQfGbvm7+OOr5J8c6UEOtWTaeE+yxV9GCeURraXWUJON593fJ89dfvzbq8cQqKAAAAIA4CQoAAAAgToICAAAAiJtqBkVJX81Rb9e7fh19fxnvftdHa3rXOokHatPPXlfJjJmzX7ti1H5RPmdNOcOzYD61/p6wp8znaE1XWG8VFAAAAECcBAUAAAAQN1WLRyvK6sawQsnTylY5/vXdf5c4v6akFazW9165JgDGVLK3l3yv9wD2zPKurIICAAAAiJOgAAAAAOIkKAAAAIC4ZWdQHPVvjdqvA/zXaEc12nPuUxILV75XP/FaejjmGrhf4vk92jsO93kXj7WOtm1NBQUAAAAQJ0EBAAAAxElQAAAAAHHLzqC40jPWc88O15kHMIbn+7Ck567kc9856uXTI7oWMwiAlq7MueGas+8Qd62R9421rPA3rAoKAAAAIE6CAgAAAIhbpsVjhXIY2hMLfapZ+nr239ZqMxFjMB73La1p6bjPXe8QR/uG9c5JtMxY77+poAAAAADiJCgAAACAOAkKAAAAIG7qGRRn+3n0krLHUU5j6PE4NrEyl3cxZb3n5lhZ0sRZXXe8J1xZs1pHq4ubcj28Qx6ZcU1VUAAAAABxEhQAAABA3NAtHo4OpYZaR0LRr5J1e46Hd60i4mFdPbYRAfeo9Zw4+t6SMn7PoutKfveffs4VR59T8vwRG2Nafd1UUAAAAABxEhQAAABAnAQFAAAAENf9DIoe+sCAdZT0fQJrMoNkbiXrWfN7zZXogxkPazs7t+zK5/A3FRQAAABAnAQFAAAAECdBAQAAAMR1N4PCzAkAevf6rPL8WYv1n8/R/Ida6y1OxtT7uvV+fTPzu29DBQUAAAAQJ0EBAAAAxHXX4vGOUhoAEhwlyR/eRebjiGmAPqigAAAAAOIkKAAAAIA4CQoAAAAgrrsZFPr8uJuYAz5hrwAAaEsFBQAAABAnQQEAAADESVAAAAAAcRIUAAAAQJwEBQAAABAnQQEAAADESVAAAAAAcRIUAAAAQJwEBQAAABAnQQEAAADE/Sj8/l9fX18/W1wIp/0nfQG/iY3+iA32iA32iA32iA2O9BAfYqNPYoM938bGY9u2uy8EAAAA4C9aPAAAAIA4CQoAAAAgToICAAAAiJOgAAAAAOIkKAAAAIA4CQoAAAAgToICAAAAiJOgAAAAAOIkKAAAAIA4CQoAAAAgToICAAAAiJOgAAAAAOIkKAAAAIA4CQoAAAAgToICAAAAiJOgAAAAAOIkKAAAAIA4CQoAAAAgToICAAAAiJOgAAAAAOIkKAAAAIA4CQoAAAAgToICAAAAiJOgAAAAAOIkKAAAAIA4CQoAAAAgToICAAAAiJOgAAAAAOJ+lHzz4/HYWl0I523b9khfg9jok9hgj9hgj9hgj9jgwK9t2/5JXoDY6JbYYM+3saGCAgAAuOJn+gLolthgz7exIUEBAAAAxElQAAAAAHESFAAAAECcBAUAAAAQJ0EBAAAAxElQAAAAAHESFAAAAECcBAUAAAAQJ0EBAAAAxElQAAAAAHESFAAAAECcBAUAAAAQ9yN9Ab3Ytu3//+/H4/Hx9x559zn06dP1/Y41z7iyZiWsL9DS615mzwHgSMk78CjPFBUUAAAAQJwEBQAAABC3TItHSflLrXJxpZpjqNkecPRZ1n98M5bRcT/7xNxaPVPEBlDK3yLze17TK8+fnt5NVFAAAAAAcRIUAAAAQJwEBQAAABA33AyKu44TPFKr14c+vPZVtVrTnnq7gLbO7iP6hftxNP/Bs59PefbziZpH3B99llla86n1POppvVVQAAAAAHESFAAAAEDccC0eZ7UqW1H2CXnv7sOj+7+k/Nb9vbbE+ivHvU+tsmh4dtQWrP1jbbXayM4+J7QUzmeW91YVFAAAAECcBAUAAAAQJ0EBAAAAxHU/g6LmsTstjNrbw76z8wpq/QyuK/n9Xvnekp51a94/a0YNJXHzLua8Y8zlXWw8r7f9aC3v7nXrzx8rzEtSQQEAAADESVAAAAAAcRIUAAAAQFz3Myh67Lmapb9nZSVnj9f6XtZ2dN45OTP1coqx+9T6/XqGrM16z633OXqvzD3p04r7hAoKAAAAIE6CAgAAAIjrvsWjByWlNcqhcmqVQJV8jtiYz4qldNzrqKy/Vsm/Ut1yd/2OPDf4hLUfU2/3d2/XQ7kr7wWjrqkKCgAAACBOggIAAACIk6AAAAAA4syg+M1RkuN7XqfUGo3a6zWbHu9RsZExwl7Qw95FO63mI9lTIK/k7wfgMyooAAAAgDgJCgAAACBOggIAAACIW2YGxZUeUD3BY0isk/7CPvR4j4qNnJH3AjOPxtdqzewpYzCTYD4l93QPaywG53JX/D3/nHScqKAAAAAA4iQoAAAAgLhlWjyUzc6v1VF96TIn3uvh/hYna2m13rViVzxe18N7Qk8lt3yvJE6ufK/1z+n9d6+lYz6rv8eqoAAAAADiJCgAAACAOAkKAAAAIG6ZGRQl3vXgHPUF6RedjzUdT8k61erze/c5Ymc8d61ZD7MO6H8dzCQY35Vnk/XnD+8b8+nh+dPT3zsqKAAAAIA4CQoAAAAgToICAAAAiFt2BkW6t4a2Xte31ZwBcTS+K2t49kx7cUML4uqao+fGXfvE2c+19jmtfvfWdC1X9gmxspY71jv9944KCgAAACBOggIAAACIW7bFo0RJ2ZUyqz61avmAM9Klc7MZrTTW/jOGWrFx9DligbO0+PShh3v46BrEBp/qKVZUUAAAAABxEhQAAABAnAQFAAAAEGcGxTd66CejrVo9wfr+1qa3fHx39HG3igV7zPjMR4L+1ZpFdzR/yr0/P3NjPqeCAgAAAIiToAAAAADiJCgAAACAODMofjvb+6WHaD5XeoLFA3+UxJG+xPm06CcWGzwTD9BGq/e+kveCks/99P2i9HO5pmRd+JsKCgAAACBOggIAAACIu63Fo7cS5iulNT1c/6xKyt2uHNfU6njI3uKc+1wpoxQr17Q6qvHKfkQfHAXNLOwxbY28VzjyvE+13hN6j78WVFAAAAAAcRIUAAAAQJwEBQAAABA31TGj+qzG9Om6vfu+kvUXK5whxubWw/qu2Gva0lEP8JVZRVfoQ6YG8XBNqz2896PpzUpjBCooAAAAgDgJCgAAACAu0uLR49FtypxySo4H7Z046sPoccQ1o+0p9o12ah0bnSI21nXl6GreO/p7o9WxndZwLaPFSk/tPyooAAAAgDgJCgAAACBOggIAAACI6+6Y0R56PoGxpWbZHBltLsIseoiFdC/nynpY/xJihT/EQltHMz5KZuX1brQ9cCZ+9+epoAAAAADiJCgAAACAOAkKAAAAIO62GRR39F+P1hfGv5WcSy2O+NTR/lPrvPOSuBFjOX73a6t1v9f6mayl5PlDW0e/+5nXZeb/tpH1sC49XMMfKigAAACAOAkKAAAAIC5yzGhPJST0bdUSPNrRigF8x/3OGa1aAYG5uP8/p4ICAAAAiJOgAAAAAOIkKAAAAIC4yAwKAACYnb5zgDIqKAAAAIA4CQoAAAAgToICAAAAiDODAgAATjBjAqAuFRQAAABAnAQFAAAAECdBAQAAAMRJUAAAAABxEhQAAABAnAQFAAAAEFd6zOivr6+vny0uhNP+k76A38RGf8QGe8QGe8QGe8QGR3qID7HRJ7HBnm9j47Ft290XAgAAAPAXLR4AAABAnAQFAAAAECdBAQAAAMRJUAAAAABxEhQAAABAnAQFAAAAECdBAQAAAMRJUAAAAABxEhQAAABAnAQFAAAAECdBAQAAAMRJUAAAAABxEhQAAABAnAQFAAAAECdBAQAAAMRJUAAAAABxEhQAAABAnAQFAAAAECdBAQAAAMRJUAAAAABxEhQAAABAnAQFAAAAECdBAQAAAMRJUAAAAABxEhQAAABAnAQFAAAAECdBAQAAAMRJUAAAAABxP0q++fF4bK0uhPO2bXukr0Fs9ElssEdssEdssEdscODXtm3/JC9AbHRLbLDn29hQQQEAAFzxM30BdEtssOfb2JCgAAAAAOIkKAAAAIA4CQoAAAAgToICAAAAiJOgAAAAAOIkKAAAAIA4CQoAAAAgToICAAAAiJOgAAAAAOIkKAAAAIA4CQoAAAAgToICAAAAiPuRvgAAGM22bX/9/8fjcfj1T7/39WusrSSOWJc4AWaiggIAAACIk6AAAAAA4rR4nKAct09HJY6vntftXak2nKHkdgxnWzFKPqfke+1HaymJm6N/K05gHe/2DfvBWmZ831RBAQAAAMRJUAAAAABxEhQAAABAXLMZFDP10V7pEaVPR727M8UufRJj/TiaR9PD3j9jbyn/UzLn5Ig9BWANPbybtKaCAgAAAIiToAAAAADibjtmdNYyVWWV/Tj7u39XKnX2+FLu0+Ma1Srd5j7vYqPVsY7K+tdyx14gNubT43OOPljvtazwfqmCAgAAAIiToAAAAADiJCgAAACAuGYzKFbojwHYY8+bT6s+36PPFUfju7KGR8fgfvrvmIP36rVZb1aiggIAAACIk6AAAAAA4iQoAAAAgLhmMyhKtDpbvpaSvr/e/1v4t5J1etcDaP3vox8b6FGrvensu0jpz6FPR+ttfddivcdU6x5eYR6JCgoAAAAgToICAAAAiOuixeNZj2WJK5TSUEcP8QrAvbScUZt3z7Vp4RpfyT1sTf+mggIAAACIk6AAAAAA4iQoAAAAgLjuZlBAb/SB9m+0Xr3Rrpe27DHjsWa04ChRPiEW5mNN/6aCAgAAAIiToAAAAADiJCgAAACAuNtmUDz31szcu6mHaHx6QPs0077hvGtqEDf3ubL/1FqnmfZAgJW1en6XfG7Pf++ooAAAAADiJCgAAACAuMgxo69lI0clJonyE2WU87PGcxmtZaL366Mt+w9nlLw72WPGYC8A7vS85/T8nFBBAQAAAMRJUAAAAABxEhQAAABAXGQGxauSI0h7nzsxSm8Pn7OOfThah1Z9vCWzLfQSs0ds9Ond/V2ybnc8J8TRWrx7rCX19w/tHD1Teni/7Hl+mwoKAAAAIE6CAgAAAIiToAAAAADiuphB8exdD2hixsOVvlT6VDL3xFnzfTpal5I1K7mf3ftjOLv+72KjZN+oxR7Tjmc7dxNjsI4r75fPz6eSZ1Vvs5POUkEBAAAAxElQAAAAAHHdtXi8Kjmi5ejf1ix5SZT50k7Nkv+ey6Vmc/ZIppr3rGNHc87+fq/EhjUdX601vGuv//R6PXv6NUvJNVDmyjtiYqRBT1RQAAAAAHESFAAAAECcBAUAAAAQ1/0Mile1joc8+tx39CHPrWQmxYp9YSNIrEvJTBzKrbrv2n/60NvMia8v6z+j1fvOYVVXZi5e+Tm9UkEBAAAAxElQAAAAAHESFAAAAEDccDMonr3ro1m1Z5lrxA3Mw3Niba/rWzLH6g49XAN1XZkjIh7WYr3X9e7d5OjrK8ymUkEBAAAAxElQAAAAAHFDt3i806rMpbcSUa75v/buJTluG4oCqLrKW8g4+1+W594DM7BUadkiWiBBXnzOmaWskhjhkWzdeg+whjCv1P1dek8cbeuepXUzqYdjPI/WpPWfnzXmg1rgiFefN0ahgwIAAACIE1AAAAAAcQIKAAAAIG7qPSjgO2rmwUed5YLR9XYcX82z4MxxYvTprvpTG/3rYV8T5jPLXgKct+IzRgcFAAAAECegAAAAAOKMeByQbi3mXrO0S8FMSvflVc9oz4LxnBkNSrzr1dgYVmy55l7qhg+9jbjeQQcFAAAAECegAAAAAOIEFAAAAECcPSgOeJ4FKs0BOSKoX44ShXm5h+mJelyL9aYFf0NwxCx1o4MCAAAAiBNQAAAAAHECCgAAACDOHhQXGnXuZwbOKOcOK5xFDTMpPe9b3c/eKWux3hxV2tNOXfFhxc+aOigAAACAOAEFAAAAEGfEgyVoleMK6grm4X7mu9QKrakp9ryqjRlHQHRQAAAAAHECCgAAACBOQAEAAADE2YPiJDNjfbIuAADAzGb8m0cHBQAAABAnoAAAAADiBBQAAABAnIACAAAAiBNQAAAAAHECCgAAACBOQAEAAADECSgAAACAOAEFAAAAECegAAAAAOJ+VH79r7e3t59XXAiH/Zu+gHdqoz9qgz1qgz1qgz1qg5Ie6kNt9EltsOfL2nhs23b3hQAAAAB8YsQDAAAAiBNQAAAAAHECCgAAACBOQAEAAADECSgAAACAOAEFAAAAECegAAAAAOIEFAAAAECcgAIAAACIE1AAAAAAcQIKAAAAIE5AAQAAAMQJKAAAAIA4AQUAAAAQJ6AAAAAA4gQUAAAAQJyAAgAAAIgTUAAAAABxAgoAAAAgTkABAAAAxAkoAAAAgDgBBQAAABAnoAAAAADiBBQAAABAnIACAAAAiBNQAAAAAHECCgAAACBOQAEAAADE/aj54sfjsV11IRy3bdsjfQ1qo09qgz1qgz1qgz1qg4Jf27b9k7wAtdEttcGeL2tDBwUAAHDGz/QF0C21wZ4va0NAAQAAAMQJKAAAAIA4AQUAAAAQJ6AAAAAA4gQUAAAAQJyAAgAAAIgTUAAAAABxAgoAAAAgTkABAAAAxAkoAAAAgDgBBQAAABAnoAAAAADifqQvoBfbtjX5Po/Ho8n3AQDGc+bzhM8QQK3SM8czZT4rrLcOCgAAACBOQAEAAADELTPi0WqEo+bnzNJmM6OaerCOANzhz3eT9894jPjQQqs2fs+Utcyy3jooAAAAgDgBBQAAABAnoAAAAADiltmD4ow/53e+O184yxzQDFrtQWJNx2TPkbmdWd+afYNWONqLLHU0hqv2NbOPGUcc/TuFMay4njooAAAAgDgBBQAAABBnxONdTSvd89dq+R3Td9fw1b8b+ejTmXY44yDjqWlvPfpvr6gbjlALY7jqyEfWlqgHY0SMQAcFAAAAECegAAAAAOIEFAAAAEDcsntQtJq7Ks0+25+gH373c0vN9R79uerxWjV7zCR4N/BBLfTpqudGy/0q1Mp4enwf0b8Vj5HVQQEAAADECSgAAACAOAEFAAAAELfMHhRXzeqtMAc0o6vONHe+9H1q7r3SngRn1uno/d+q/njtqpnvM89+azwf+9GMb7TPcz5vjOfMXgJXrLE9cOY36hrroAAAAADiBBQAAABA3DIjHi2N1gaINeN/LdvbWo4P7H3dKO14M3j1u/YcYU/vR9vyt97HtGrGAbw3xnDXs8HzaHyt1m3UZ4EOCgAAACBOQAEAAADECSgAAACAOHtQvFt91mc26aObaG/0NS1dk9liGNvozyfKelizM3tSlL4P1+lhn5MzR5uSc/Qz4yx0UAAAAABxAgoAAAAgTkABAAAAxC2zB8VV8zpm+frUw8ydvQO4wnNdqam27npOeDaM70ytuIdzVpjdJutojXkW8Gz1OtJBAQAAAMQJKAAAAIC4qUY8ao5V6uHoH3Ic3zSX0e/J5+tXQ+vR8j+eM89+a5zz3WftCGvk6Orx9Pa77u16OG+W+1sHBQAAABAnoAAAAADiBBQAAABA3FR7UJRmQs2HrsUcP6OqmW2fZdawV4k9ZkafiwdYSc17wTubPf5u+UwHBQAAABAnoAAAAADiBBQAAABA3FR7ULRiRmw+1nBuM9+ziX0QVlKzx8ezVzV2dJ2sN1zHHi+cdeaZfFeNeW+sa5bnmA4KAAAAIE5AAQAAAMRNPeJRanM5cyzQd38G93pep1dt0ol1K10fr616/Kbnz7Wu+h0eff/MVLvQm+f7SRv8bz6b1JlpDM/7ph8z1VULOigAAACAOAEFAAAAECegAAAAAOKm3oOi5MysjxmtPpVmS+9aM/OtfeptH4dSfb6qG88fAEp8FrlOj+/oVsdaQy90UAAAAABxAgoAAAAgTkABAAAAxC27B8WZmTxnRvevx3Xp8ZpG1mrG9sz+M61me3vbI4Nr1dSR9w20417720r/r3e4qo5afd6w3n2yV8xnOigAAACAOAEFAAAAEBcZ8bjqSM8zR/e1ugZyemiTX7VFNO3V7/qOUYxW1M18vH945j2R4/hNrtbqnjbSMTefC8p0UAAAAABxAgoAAAAgTkABAAAAxEX2oDh6xFot84VrMVvKntK8XqJWVpgfhJm1mh9+9X08K67T6rNoYo8r7lXzu/eZgu+46tj6WeigAAAAAOIEFAAAAEBc98eM3kV71PgSddVjLVPHvU9P1OMYzrTnem/Mpff11Ep+Xm8jxN4TzE4HBQAAABAnoAAAAADiBBQAAABAXPfHjF71M5lPaUbwz/+uqYea+lRnMK/nZ8Gre72HOWX6VHpXeYfklH73PRxVXfrsXFM3amxM1m0uPiOU6aAAAAAA4gQUAAAAQJyAAgAAAIiL7EFxFfNZfHi1z4nZL6BWae+AVt+XtVj78aXWUO1ktNxHzxquq6aOVqwTHRQAAABAnIACAAAAiOtixGPF1hXuVXM8lzYr4Cutxjo8R+ZjTWFN7n1aUEef6aAAAAAA4gQUAAAAQJyAAgAAAIjrYg8KuFtp1sscGPCVq44ZBQDgNx0UAAAAQJyAAgAAAIgTUAAAAABx9qAAgEr2qgEAaE8HBQAAABAnoAAAAADiBBQAAABAnIACAAAAiBNQAAAAAHECCgAAACCu9pjRX29vbz+vuBAO+zd9Ae/URn/UBnvUBnvUBnvUBiU91Ifa6JPaYM+XtfHYtu3uCwEAAAD4xIgHAAAAECegAAAAAOIEFAAAAECcgAIAAACIE1AAAAAAcQIKAAAAIE5AAQAAAMQJKAAAAIA4AQUAAAAQ9x+dvVQ9BAL+jAAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 1080x1080 with 64 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5-Qm0DN-Kko8"
      },
      "source": [
        "### Torch auxiliary modules"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qnuEiSNKKko8"
      },
      "source": [
        "class Flatten(torch.nn.Module):\n",
        "    def forward(self, input):\n",
        "        return input.view(input.size(0), -1)\n",
        "    \n",
        "class UnFlatten(torch.nn.Module):\n",
        "    def forward(self, input, size=1024):\n",
        "        return input.view(input.size(0), size, 1, 1)\n",
        "\n",
        "# Debugging module \n",
        "class NNprint(torch.nn.Module):\n",
        "    def forward(self, input):\n",
        "        if NNprint_==True:\n",
        "            print(input.shape)\n",
        "        return input  "
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nck2uJ6PKko8"
      },
      "source": [
        "## VAE"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jZJT7x8qKko8"
      },
      "source": [
        "#VAE implementation"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZoMdnAHnKko8"
      },
      "source": [
        "## CVAE"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aBUpmNKAKko8"
      },
      "source": [
        "\n",
        "class CVAE(torch.nn.Module):\n",
        "    def __init__(self, image_channels=1, h_dim=1024, z_dim=32,num_labels=0):\n",
        "        super(CVAE, self).__init__()\n",
        "        self.encoder = torch.nn.Sequential(\n",
        "            NNprint(),\n",
        "            torch.nn.Conv2d(image_channels, 32, kernel_size=3, stride=2),\n",
        "            NNprint(),\n",
        "            torch.nn.LeakyReLU(),\n",
        "            torch.nn.Conv2d(32, 64, kernel_size=3, stride=2),\n",
        "            NNprint(),\n",
        "            torch.nn.LeakyReLU(),\n",
        "            torch.nn.Conv2d(64, 128, kernel_size=3, stride=1),\n",
        "            NNprint(),\n",
        "            torch.nn.LeakyReLU(),\n",
        "            torch.nn.Conv2d(128, 256, kernel_size=3, stride=1),\n",
        "            NNprint(),\n",
        "            torch.nn.LeakyReLU(),\n",
        "            Flatten(),\n",
        "            \n",
        "            \n",
        "            NNprint(),\n",
        "            \n",
        "        )\n",
        "        self.h_dim=h_dim\n",
        "        self.num_labels=num_labels\n",
        "        self.fc1 = torch.nn.Linear(h_dim, z_dim)\n",
        "        self.fc2 = torch.nn.Linear(h_dim, z_dim)\n",
        "        self.fc3 = torch.nn.Linear(z_dim+num_labels, h_dim)\n",
        "        \n",
        "        self.decoder = torch.nn.Sequential(\n",
        "            NNprint(),\n",
        "            UnFlatten(),\n",
        "            NNprint(),\n",
        "            torch.nn.ConvTranspose2d(h_dim, 128, kernel_size=4, stride=2),\n",
        "            torch.nn.LeakyReLU(),\n",
        "            NNprint(),\n",
        "            torch.nn.ConvTranspose2d(128, 64, kernel_size=5, stride=2),\n",
        "            torch.nn.LeakyReLU(),\n",
        "            NNprint(),\n",
        "            torch.nn.ConvTranspose2d(64, 32, kernel_size=4,padding=0, stride=2),\n",
        "            torch.nn.LeakyReLU(),\n",
        "            NNprint(),\n",
        "            torch.nn.ConvTranspose2d(32, image_channels, kernel_size=5, stride=1),\n",
        "            torch.nn.Sigmoid(),\n",
        "            NNprint(),\n",
        "        )\n",
        "        \n",
        "    def reparameterize(self, mu, logvar):\n",
        "        std = logvar.mul(0.5).exp_()\n",
        "        # return torch.normal(mu, std)\n",
        "        esp = torch.randn(*mu.size())\n",
        "        z = mu.to(device) + std.to(device) * esp.to(device)\n",
        "        return z\n",
        "    \n",
        "    def bottleneck(self, h,labels):\n",
        "        mu, logvar = self.fc1(h), self.fc2(h)\n",
        "        z = self.reparameterize(mu, logvar)\n",
        "        if self.num_labels>0:\n",
        "            z=torch.cat((z,torch.nn.functional.one_hot(labels,self.num_labels).type(torch.float).to(device)),1)\n",
        "        return z, mu, logvar\n",
        "\n",
        "    def encode(self, x,labels):\n",
        "        h = self.encoder(x)\n",
        "#         h=torch.cat((h,labels.float().reshape(labels.size(0),1).to(device)),dim=1)\n",
        "        z, mu, logvar = self.bottleneck(h,labels)\n",
        "        return z, mu, logvar\n",
        "\n",
        "    def decode(self, z):\n",
        "        z = self.fc3(z)\n",
        "        z = self.decoder(z)\n",
        "        return z\n",
        "\n",
        "    \n",
        "    \n",
        "    def forward(self, x,labels):\n",
        "        z, mu, logvar = self.encode(x,labels)\n",
        "#         print('z',z.shape)\n",
        "        z = self.decode(z)\n",
        "        return z, mu, logvar\n",
        "    \n",
        "    def elbo(self,recon_x, x, mu, logvar):\n",
        "        BCE = F.binary_cross_entropy(recon_x, x.to(device), size_average=False,reduction='sum' )\n",
        "        # BCE = F.mse_loss(recon_x, x, size_average=False)\n",
        "\n",
        "        # see Appendix B from VAE paper:\n",
        "        # Kingma and Welling. Auto-Encoding Variational Bayes. ICLR, 2014\n",
        "        # 0.5 * sum(1 + log(sigma^2) - mu^2 - sigma^2)\n",
        "        KLD = 0.5 * torch.mean(1 + logvar - mu.pow(2) - logvar.exp())\n",
        "        return BCE - KLD, BCE, KLD\n",
        "\n",
        "    def sample(self,z):\n",
        "        return self.decode(z)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H6Uw0znDKko8",
        "outputId": "e5aff56c-0cec-4d75-b419-c2cde377c0b8"
      },
      "source": [
        "image_channels = example_data.size(1)\n",
        "image_channels"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A4ZDcGOSKko9"
      },
      "source": [
        "model = CVAE(image_channels=image_channels,h_dim=1024,z_dim=latent_dim,num_labels=num_classes).to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oJVZdVuqKko9"
      },
      "source": [
        "def fit_model(model, train_loader):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    # Run each batch in training dataset\n",
        "    for idx, (images, labels) in enumerate(train_loader):\n",
        "        recon_images, mu, logvar = model(images.to(device),labels)\n",
        "        loss, bce, kld = model.elbo(recon_images, images, mu, logvar)\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss\n",
        "        \n",
        "    return running_loss/len(train_loader.dataset)\n",
        "    \n",
        "\n",
        "def test_model(model, test_loader):\n",
        "    model.eval()\n",
        "    running_loss = 0.0\n",
        "    with torch.no_grad():\n",
        "        for idx, (images, labels) in enumerate(test_loader):\n",
        "            recon_images, mu, logvar = model(images.to(device),labels)\n",
        "            loss, bce, kld = model.elbo(recon_images, images, mu, logvar)\n",
        "            running_loss += loss\n",
        "\n",
        "    return running_loss/len(test_loader.dataset)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cJney_KTKko9",
        "outputId": "21f62428-30ec-4011-9f2b-f4a2bc2bdb87"
      },
      "source": [
        "%%time\n",
        "##\n",
        "train_loss = []\n",
        "test_loss = []\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    print(f\"Epoch {epoch+1} of {epochs}\")\n",
        "    train_epoch_loss = fit_model(model, train_loader)\n",
        "    test_epoch_loss = test_model(model, test_loader)\n",
        "    train_loss.append(train_epoch_loss)\n",
        "    test_loss.append(test_epoch_loss)\n",
        "    print(f\"Train Loss: {train_epoch_loss:.4f}\")\n",
        "    print(f\"Val Loss: {test_epoch_loss:.4f}\")\n",
        "\n",
        "torch.save(model.state_dict(), 'cvae.torch')\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1 of 5\n",
            "Train Loss: 122.4315\n",
            "Val Loss: 122.2494\n",
            "Epoch 2 of 5\n",
            "Train Loss: 119.7727\n",
            "Val Loss: 119.6592\n",
            "Epoch 3 of 5\n",
            "Train Loss: 117.7296\n",
            "Val Loss: 118.1562\n",
            "Epoch 4 of 5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BYKGI7HbKko9"
      },
      "source": [
        "plot_examples()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fjbG1dMmKko9"
      },
      "source": [
        "#Reconstructions\n",
        "plot_examples(model)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "5sQAVtTQKko-"
      },
      "source": [
        "# Sample latent space\n",
        "plot_examples(model,sample=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OcDwVWZ4LQz9"
      },
      "source": [
        "# 2.3  Semi-Supervised VAE (M1)\n",
        "\n",
        "We have now trained a VAE on the full unlabeled dataset.  We will now implement the M1 model from Semi-Supervised Learning with Deep Generative Models. This amounts to simply traininga classifier on top of the representations learned by our VAE.\n",
        "\n",
        "1. Extract 100 labeled datapoints from MNIST (10 from each class).\n",
        "2.(Report) Train a classifier on their latent representation.  First try a linear classifier and then experiment with other types of classifiers.  Report the accuracies.\n",
        "3.(Report) Implement  some  simple  baselines:  Train  a  linear  classifier  or  neural  network  from scratch using only the 100 labeled data points.  Report the accuracies."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vA8nlOm0bpb5"
      },
      "source": [
        "# 0 Train VAE on all available data (X), without labels (y)\n",
        "\n",
        "model = CVAE(image_channels=image_channels,h_dim=1024,z_dim=latent_dim,num_labels=0).to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dVjsLiA7buPi",
        "outputId": "f65362be-9235-4714-967c-83c6cf227b6f"
      },
      "source": [
        "##\n",
        "train_loss = []\n",
        "test_loss = []\n",
        "epochs=5\n",
        "learning_rate=1e-3\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    print(f\"Epoch {epoch+1} of {epochs}\")\n",
        "    train_epoch_loss = fit_model(model, train_loader)\n",
        "    test_epoch_loss = test_model(model, test_loader)\n",
        "    train_loss.append(train_epoch_loss)\n",
        "    test_loss.append(test_epoch_loss)\n",
        "    print(f\"Train Loss: {train_epoch_loss:.4f}\")\n",
        "    print(f\"Val Loss: {test_epoch_loss:.4f}\")\n",
        "\n",
        "# torch.save(model.state_dict(), 'cvae.torch')"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1 of 5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/_reduction.py:44: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
            "  warnings.warn(warning.format(ret))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train Loss: 117.6458\n",
            "Val Loss: 91.3892\n",
            "Epoch 2 of 5\n",
            "Train Loss: 80.4845\n",
            "Val Loss: 64.7040\n",
            "Epoch 3 of 5\n",
            "Train Loss: 55.2358\n",
            "Val Loss: 48.9142\n",
            "Epoch 4 of 5\n",
            "Train Loss: 47.4716\n",
            "Val Loss: 45.3025\n",
            "Epoch 5 of 5\n",
            "Train Loss: 44.6698\n",
            "Val Loss: 43.1558\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JHPj9_YxLs2E"
      },
      "source": [
        "# 1 Extract 100 labeled datapoints from MNIST (10 from each class).\n",
        "\n",
        "def get_target_indexes(dset, n_class_examples):\n",
        "  idxs = []\n",
        "  ts = []\n",
        "  n_samples = n_class_examples * len(np.unique(dset_train.targets.numpy()))\n",
        "\n",
        "  for j, x in enumerate(dset.targets):\n",
        "    if (ts.count(x.numpy()) < n_class_examples):\n",
        "      ts.append(x.numpy())\n",
        "      idxs.append(j)\n",
        "\n",
        "    if len(idxs) == n_samples:\n",
        "      break\n",
        "\n",
        "  return idxs\n",
        "\n",
        "# np.unique(dset_train.targets[idxs].numpy(), return_counts=True)\n",
        "# torch.utils.data.Subset(dset_train, idxs)"
      ],
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2epGrH1beyzU"
      },
      "source": [
        "idx_train_subset = get_target_indexes(dset_train, 10)\n",
        "dset_train_subset = torch.utils.data.Subset(dset_train, idx_train_subset)\n",
        "\n",
        "dset_train_subset_loader = torch.utils.data.DataLoader(\n",
        "  dset_train_subset,\n",
        "  batch_size=1000)"
      ],
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NEL7JExweLGv"
      },
      "source": [
        "idx_test_subset = get_target_indexes(dset_test, 100)\n",
        "dset_test_subset = torch.utils.data.Subset(dset_test, idx_test_subset)\n",
        "\n",
        "dset_test_subset_loader = torch.utils.data.DataLoader(\n",
        "  dset_test_subset,\n",
        "  batch_size=100)"
      ],
      "execution_count": 95,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ojv6XFmtLz2q",
        "outputId": "344ff440-5f45-4f67-9d5e-953a5a34a053"
      },
      "source": [
        "# 2 (Report) Train a classifier on their latent representation. \n",
        "# First try a linear classifier and then experiment with other types of classifiers. Report the accuracies.\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "model.eval()\n",
        "\n",
        "X_train = None\n",
        "\n",
        "with torch.no_grad():\n",
        "    for idx, (images, labels) in enumerate(dset_train_subset_loader):\n",
        "        recon_images, mu, logvar = model(images.to(device), None)\n",
        "        X_train = mu.numpy()\n",
        "\n",
        "y_train = dset_train.targets[idx_train_subset]\n",
        "clf = LogisticRegression(random_state=0).fit(X_train, y_train)\n",
        "\n",
        "print(\"Loss (train): \", clf.score(X_train, y_train))"
      ],
      "execution_count": 105,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loss (train):  0.94\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KBW2i2nml-2C",
        "outputId": "5cfe9ce1-4554-408e-c6fc-1728110e1a0f"
      },
      "source": [
        "X_test = None\n",
        "err = []\n",
        "with torch.no_grad():\n",
        "    for idx, (images, labels) in enumerate(dset_test_subset_loader):\n",
        "        recon_images, mu, logvar = model(images.to(device), None)\n",
        "        X_test = mu.numpy()\n",
        "        y_test = labels.numpy()\n",
        "        err.append(clf.score(X_test, y_test))\n",
        "\n",
        "print(\"Mean loss (test): \", np.mean(np.array(err)))"
      ],
      "execution_count": 106,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.66, 0.59, 0.68, 0.58, 0.64, 0.62, 0.56, 0.69, 0.66, 0.54]\n",
            "Mean loss (test):  0.622\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V82DkTSImjVn"
      },
      "source": [
        ""
      ],
      "execution_count": 100,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SmY2NIWVKko-"
      },
      "source": [
        "# \n",
        "\n",
        "z1 = torch.randn(1, latent_dim).to(device)\n",
        "z2 = torch.randn(1, latent_dim).to(device)\n",
        "z3 = torch.randn(1, latent_dim).to(device)\n",
        "z4 = torch.randn(1, latent_dim).to(device)\n",
        "\n",
        "y = torch.randint(0, 10,(1,1)).to(dtype=torch.long) # 10=number of classes\n",
        "y = torch.nn.functional.one_hot(y,10).type(torch.float).to(device,dtype=z.dtype)[0]\n",
        "\n",
        "z1 = torch.cat((z1,y),dim=1)\n",
        "z2 = torch.cat((z2,y),dim=1)\n",
        "z3 = torch.cat((z3,y),dim=1)\n",
        "z4 = torch.cat((z4,y),dim=1)\n",
        "\n",
        "print(z1)\n",
        "print(z2)\n",
        "print(z3)\n",
        "print(z4)\n",
        "rec1 = model.decode(z1)\n",
        "rec2 = model.decode(z2)\n",
        "rec3 = model.decode(z3)\n",
        "rec4 = model.decode(z4)\n",
        "\n",
        "img1 = rec1.view(28,28).data\n",
        "img2 = rec2.view(28,28).data\n",
        "img3 = rec3.view(28,28).data\n",
        "img4 = rec4.view(28,28).data\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cyaotocXKko-"
      },
      "source": [
        "plt.figure()\n",
        "plt.imshow(img1.to('cpu'), cmap='gray')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GGx8hr_uKko-"
      },
      "source": [
        "plt.figure()\n",
        "plt.imshow(img2.to('cpu'), cmap='gray')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iC6bN4D3Kko-"
      },
      "source": [
        "plt.figure()\n",
        "plt.imshow(img3.to('cpu'), cmap='gray')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kMFNw4oJKko-"
      },
      "source": [
        "plt.figure()\n",
        "plt.imshow(img4.to('cpu'), cmap='gray')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GOJBem2uKko_"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5qD30Z3xKko_"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}